Here are the **design aspects worth discussing** (before implementation) that will make this project feel “real,” defensible, and successful—especially for decision-makers + a portfolio.

## 1) Data trust, licensing, and “what we promise”

Even “free public” data can have **terms about redistribution** and attribution.

* Do we store the raw dataset, or only store normalized facts + reference links?
* How do we display attribution consistently per source/metric?
* What’s our policy when a source changes schema, gets deprecated, or restricts access?
* Do we publish a **Data Use Policy** and “Known limitations” list?

**Why it matters:** credibility and future SaaS viability.

## 2) Coverage model and user expectation management

Your biggest UX risk is: users will assume drilldown means data exists.

* How do we **visualize coverage** before they drill down?
* Do we show a “Coverage Matrix” per layer (STATE/COUNTY/PLACE) and per time range?
* How do we show “no data” in a way that doesn’t feel like a bug?

**Design idea:** a small “Coverage badge” next to the layer selector: `STATE only`, `COUNTY+`, etc.

## 3) “Compare sources” philosophy

You said: show all sources, don’t blend. Great—but you need clear rules:

* When two sources disagree, do we:

  * show both (yes),
  * show a warning “values differ across sources,”
  * show a “why they differ” explainer (definitions/units/coverage)?
* Do we allow side-by-side charts and map toggles?

**Why it matters:** reduces confusion and avoids “which is correct?” debates.

## 4) Metric catalog design (this is the product)

Metrics aren’t just numbers—they’re definitions.
For each metric decide:

* definition in plain English
* unit
* sector/filter (e.g., electricity residential vs commercial)
* geographic support
* temporal granularity
* source(s) mapping
* what “good/bad” means (optional; careful: can be opinionated)

**Why it matters:** this becomes your “API product” and prevents future rewrites.

## 5) Time semantics and “freshness”

You need a crisp policy:

* Do we show the **latest available period** by default?
* If data is monthly and we’re in early January, “latest” might be November.
* How do we pick “latest common period” for cross-layer queries?
* How do we handle revisions (source updates a prior month)?

**Design decision to make:** “We prefer most recently *published* period, not most recently *retrieved*.”

## 6) Data versioning and reproducibility

To defend integrity, decide how far you go:

* keep only latest value per period, or keep “versions” when revised?
* store raw payload hash always?
* allow a user to reproduce an answer from a specific ingestion run (“as-of” querying)?

**Minimum recommended:** keep `source_run` history + payload hash; update facts in-place for the same period.

## 7) Geo boundaries: what is a “city”?

People will argue about city vs ZIP vs metro vs municipality.

* are you committing to **Census Place** as “city”?
* do you want additional layers later: ZIP Code Tabulation Areas (ZCTA), metro areas (CBSA), utility service territories?

**Design choice:** start with Place; document it clearly on the transparency page.

## 8) UX for “analysis,” not just “map coloring”

Decision-makers want answers, not just colors.
Consider:

* “Top 10 / Bottom 10” quick panels
* “Compare selected regions” (2–5 regions) chart view
* “Scenario view” (cross-layer intersection) with transparent thresholds
* Export formats and what columns they include (provenance columns must be in exports)

## 9) Util Agent scope boundaries (to prevent trust issues)

Even read-only can cause trust issues if it sounds too confident.
Decide:

* Util Agent response tone: always shows citations, always shows “data coverage used”
* Does it ever answer with partial data? (e.g., electricity exists but broadband missing)
* Should it ask follow-ups or just refuse?
* Do you want “explain how you computed this” always available?

**Recommended policy:** default to strict intersection; allow a “show partial results” toggle.

## 10) Performance and precomputation strategy

Maps can be heavy if computed on the fly.
Decide early:

* will you compute legend stats (min/max/percentiles) per request or store precomputed?
* will you cache map responses by `(metric, source, geoLevel, parent, period)`?
* how many years of history will you keep?

**Portfolio-friendly:** keep it simple but define caching strategy in docs.

## 11) Operational model: ingestion reliability and observability

Since this is data-driven, ingestion quality is the product.
Discuss:

* how you detect failures and display them publicly
* retries/backoff policy
* “source down” vs “schema changed” classification
* a public “Data Status” page (you already want it—great)

## 12) Governance and contribution model

If you put this on GitHub:

* will you accept PRs for new sources?
* how do you validate sources (must be public, must have terms link, must have stable identifiers)?
* what’s your definition of “done” for adding a layer?

## 13) Product positioning and audience

Even if it’s portfolio-first, define:

* your primary “buyer persona” (CRE/site selection vs public sector vs consulting)
* the top 3 questions your dashboard answers quickly
  This influences which metrics you pick first.

---

### The 5 design decisions I’d lock next (best ROI)

1. **Metric catalog v1**: which 4–8 metrics, exact definitions, units, and supported geos
2. **Freshness policy**: “latest available” rules + latest common period for cross-layer
3. **Source disagreement UX**: compare mode + how to explain differences
4. **“City” definition**: Census Place + how it’s explained on transparency page
5. **Util Agent strictness**: intersection-only by default + optional partial toggle

If you want, I can propose a **default set of choices** for all five (with reasoning) so you don’t have to decide from scratch—then you can adjust only what you disagree with.
